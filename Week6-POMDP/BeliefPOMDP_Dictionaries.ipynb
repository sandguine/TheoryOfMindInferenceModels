{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:02.877288Z",
     "start_time": "2019-02-12T15:53:02.731504Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:02.880585Z",
     "start_time": "2019-02-12T15:53:02.878477Z"
    }
   },
   "outputs": [],
   "source": [
    "from setupPOMDP import *\n",
    "from visualizeEnvironment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three possible environments: C, Reverse C, and O shaped (letter corresponds to where the barrier is)  \n",
    "\n",
    "environmentC, environmentReverseC, and environmentO are the environmental dynamics for each environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:02.885519Z",
     "start_time": "2019-02-12T15:53:02.881888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actions correspond to E, N, W, S, Stay respectively\n",
    "allActions = [(1,0), (0,1), (-1,0), (0,-1), (0,0)]\n",
    "\n",
    "#all location states in grid\n",
    "gridWidth = 10\n",
    "gridHeight = 5\n",
    "gridSet = set(itertools.product(range(gridWidth), range(gridHeight)))\n",
    "\n",
    "#set of states to remove from each environment\n",
    "barriersC = {(2,2), (3,2), (4,2), (5,2), (6,2), (7,2), (8,2), (9,2)}\n",
    "barriersReverseC = {(0,2), (1,2), (2,2), (3,2), (4,2), (5,2), (6,2), (7,2)}\n",
    "barriersO = {(2,2), (3,2), (4,2), (5,2), (6,2), (7,2)}\n",
    "\n",
    "#final environment state sets\n",
    "stateSetC = list(gridSet.difference(barriersC))\n",
    "stateSetReverseC = list(gridSet.difference(barriersReverseC))\n",
    "stateSetO = list(gridSet.difference(barriersO))\n",
    "\n",
    "#goal locations\n",
    "goal1 = (0,0)\n",
    "goal2 = (9,4)\n",
    "\n",
    "#probability of ineffective action\n",
    "epsilon = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:02.894089Z",
     "start_time": "2019-02-12T15:53:02.886519Z"
    }
   },
   "outputs": [],
   "source": [
    "getTransition = SetupTransition(epsilon)\n",
    "\n",
    "# possible environment transitions\n",
    "environmentC = getTransition(stateSetC, allActions)\n",
    "environmentReverseC = getTransition(stateSetReverseC, allActions)\n",
    "environmentO = getTransition(stateSetO, allActions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:02.897916Z",
     "start_time": "2019-02-12T15:53:02.895231Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Probabilistically draw the next location state given the dynamics of the transition function\n",
    "Input: transition dictionary, current state tuple, action tuple\n",
    "Output: next location state tuple\n",
    "\"\"\"\n",
    "def drawNextState(transitionFunction, state, action):\n",
    "    nextStates = list(transitionFunction[state][action].keys())\n",
    "    probabilities = list(transitionFunction[state][action].values())\n",
    "    nextState = nextStates[np.random.choice(len(nextStates), p=probabilities)]\n",
    "    return(nextState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:02.902786Z",
     "start_time": "2019-02-12T15:53:02.898964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "drawNextState(environmentC, (0,0), (1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment C Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:03.053991Z",
     "start_time": "2019-02-12T15:53:02.903886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualizeEnvironment(gridWidth = gridWidth, gridHeight = gridHeight, states = stateSetC, \\\n",
    "                     goalStates = [(0,0, \" \"), (9,4, \" \")], trajectory = [(7,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lists all possible worlds as well as all possible preference rankings among K, L, and M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:03.057511Z",
     "start_time": "2019-02-12T15:53:03.055232Z"
    }
   },
   "outputs": [],
   "source": [
    "allWorlds = [(c1, c2) for c1, c2 in itertools.permutations(\"KLM\", 2)]\n",
    "possibleWorldsInLocation = [{goal1: comb1, goal2: comb2} for comb1, comb2 in itertools.permutations(\"KLM\", 2)]\n",
    "possiblePreferenceRankings = [rank for rank in itertools.permutations(\"KLM\",3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:03.062178Z",
     "start_time": "2019-02-12T15:53:03.058764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('K', 'L'), ('K', 'M'), ('L', 'K'), ('L', 'M'), ('M', 'K'), ('M', 'L')] \n",
      "\n",
      "[{(0, 0): 'K', (9, 4): 'L'}, {(0, 0): 'K', (9, 4): 'M'}, {(0, 0): 'L', (9, 4): 'K'}, {(0, 0): 'L', (9, 4): 'M'}, {(0, 0): 'M', (9, 4): 'K'}, {(0, 0): 'M', (9, 4): 'L'}] \n",
      "\n",
      "[('K', 'L', 'M'), ('K', 'M', 'L'), ('L', 'K', 'M'), ('L', 'M', 'K'), ('M', 'K', 'L'), ('M', 'L', 'K')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(allWorlds, \"\\n\")\n",
    "print(possibleWorldsInLocation, \"\\n\")\n",
    "print(possiblePreferenceRankings, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:03.066497Z",
     "start_time": "2019-02-12T15:53:03.063324Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Construct the input for a reward function.\n",
    "\n",
    "Inputs: world is a dictionary where keys are the coordinates of goals and the values are the names of those goals\n",
    "preference is a tuple indicating the relatve ranking of the worth of goals and \n",
    "preference rewards is a list of how much reward the rank of each goal should receive\n",
    "\n",
    "outputs: a dictionary with goal locations as keys and the numerical reward associated with them as the values\n",
    "\"\"\"\n",
    "def constructPreferences(world, preference, preferenceRewards = [1000, 400, 100]):\n",
    "    goalPreferences = {location : preferenceRewards[preference.index(truck)] for location, truck in world.items()}\n",
    "    return(goalPreferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:03.070295Z",
     "start_time": "2019-02-12T15:53:03.067656Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 400, (9, 4): 100}\n",
      "{(0, 0): 400, (9, 4): 100}\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "print(constructPreferences({(0,0):'L', (9,4):'K'}, ('M', 'L', 'K')))\n",
    "print(constructPreferences(possibleWorldsInLocation[2], possiblePreferenceRankings[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards R(s, a, s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is defined for first, the environment, and second for the relative preferences of the agent. \n",
    "We create a reward for each possible world, given that preference relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:53:03.076397Z",
     "start_time": "2019-02-12T15:53:03.071697Z"
    }
   },
   "outputs": [],
   "source": [
    "getRewardC = SetupRewardTable(environmentC, allActions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:57:27.910754Z",
     "start_time": "2019-02-12T15:57:27.904479Z"
    }
   },
   "outputs": [],
   "source": [
    "# example preference of M > K > L, a true world of K in position 0,0 and L in position 9,4\n",
    "klWorld_mlkPreference = constructPreferences(possibleWorldsInLocation[2], possiblePreferenceRankings[-1])\n",
    "\n",
    "# example reward function dictionaries for environment C dynamics, Preference M > K > L \n",
    "rewards_EnvC_PrefMLK = {tuple(world.values()) : getRewardC(constructPreferences(world, possiblePreferenceRankings[-1])) \\\n",
    "                        for world in possibleWorldsInLocation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:59:09.497759Z",
     "start_time": "2019-02-12T15:59:09.453142Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (7, 3)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (8, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (7, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (6, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (7, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (7, 3)\n",
      "\t\t\treward: -1\n",
      "state: (1, 3)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (2, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (1, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (0, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (1, 2)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (1, 3)\n",
      "\t\t\treward: -1\n",
      "state: (9, 1)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (9, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (9, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (8, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (9, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (9, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (9, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (9, 1)\n",
      "\t\t\treward: -1\n",
      "state: (3, 0)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (4, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (3, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (2, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (3, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (3, 0)\n",
      "\t\t\treward: -1\n",
      "state: (0, 2)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (1, 2)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 2)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (0, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 2)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (0, 2)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (0, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 2)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (0, 2)\n",
      "\t\t\treward: -1\n",
      "state: (8, 0)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (9, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (8, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (7, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (8, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (8, 0)\n",
      "\t\t\treward: -1\n",
      "state: (2, 1)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (3, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (2, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (1, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (2, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (2, 1)\n",
      "\t\t\treward: -1\n",
      "state: (9, 4)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (9, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (9, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (8, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (9, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (9, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (9, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (9, 4)\n",
      "\t\t\treward: 100\n",
      "state: (5, 1)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (6, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (5, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (4, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (5, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (5, 1)\n",
      "\t\t\treward: -1\n",
      "state: (0, 3)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (1, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (0, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (0, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (0, 2)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (0, 3)\n",
      "\t\t\treward: -1\n",
      "state: (4, 0)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (5, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (4, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (3, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (4, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (4, 0)\n",
      "\t\t\treward: -1\n",
      "state: (1, 2)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (1, 2)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (1, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 2)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (0, 2)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 2)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (1, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 2)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (1, 2)\n",
      "\t\t\treward: -1\n",
      "state: (9, 0)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (9, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (9, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (9, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (8, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (9, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (9, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (9, 0)\n",
      "\t\t\treward: -1\n",
      "state: (3, 3)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (4, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (3, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (2, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (3, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (3, 3)\n",
      "\t\t\treward: -1\n",
      "state: (8, 1)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (9, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (8, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (7, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (8, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (8, 1)\n",
      "\t\t\treward: -1\n",
      "state: (4, 4)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (5, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (4, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (3, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (4, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (4, 4)\n",
      "\t\t\treward: -1\n",
      "state: (6, 3)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (7, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (6, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (5, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (6, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (6, 3)\n",
      "\t\t\treward: -1\n",
      "state: (5, 0)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (6, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (5, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (4, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (5, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (5, 0)\n",
      "\t\t\treward: -1\n",
      "state: (0, 4)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (1, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (0, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (0, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (0, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (0, 4)\n",
      "\t\t\treward: -1\n",
      "state: (4, 1)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (5, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (4, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (3, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (4, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (4, 1)\n",
      "\t\t\treward: -1\n",
      "state: (1, 1)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (2, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (1, 2)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (0, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (1, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (1, 1)\n",
      "\t\t\treward: -1\n",
      "state: (6, 4)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (7, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (6, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (5, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (6, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (6, 4)\n",
      "\t\t\treward: -1\n",
      "state: (0, 0)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (1, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (0, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (0, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (0, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (0, 0)\n",
      "\t\t\treward: 400\n",
      "state: (5, 4)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (6, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (5, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (4, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (5, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (5, 4)\n",
      "\t\t\treward: -1\n",
      "state: (7, 1)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (8, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (7, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (6, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (7, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (7, 1)\n",
      "\t\t\treward: -1\n",
      "state: (9, 3)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (9, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (9, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (9, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (8, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (9, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (9, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (9, 3)\n",
      "\t\t\treward: -1\n",
      "state: (6, 0)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (7, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (6, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (5, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (6, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (6, 0)\n",
      "\t\t\treward: -1\n",
      "state: (1, 4)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (2, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (1, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (0, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (1, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (1, 4)\n",
      "\t\t\treward: -1\n",
      "state: (2, 3)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (3, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (2, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (1, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (2, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (2, 3)\n",
      "\t\t\treward: -1\n",
      "state: (1, 0)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (2, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (1, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (0, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (1, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (1, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (1, 0)\n",
      "\t\t\treward: -1\n",
      "state: (5, 3)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (6, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (5, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (4, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (5, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (5, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (5, 3)\n",
      "\t\t\treward: -1\n",
      "state: (0, 1)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (1, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (0, 2)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (0, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (0, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (0, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (0, 1)\n",
      "\t\t\treward: -1\n",
      "state: (8, 3)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (9, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (8, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (7, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (8, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (8, 3)\n",
      "\t\t\treward: -1\n",
      "state: (7, 0)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (8, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (7, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (6, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (7, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (7, 0)\n",
      "\t\t\treward: -1\n",
      "state: (6, 1)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (7, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (6, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (5, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (6, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (6, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (6, 1)\n",
      "\t\t\treward: -1\n",
      "state: (3, 1)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (4, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (3, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (2, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (3, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 1)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (3, 1)\n",
      "\t\t\treward: -1\n",
      "state: (7, 4)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (8, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (7, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (6, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (7, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (7, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (7, 4)\n",
      "\t\t\treward: -1\n",
      "state: (2, 0)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (3, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (2, 1)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (1, 0)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (2, 0)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (2, 0)\n",
      "\t\t\treward: -1\n",
      "state: (4, 3)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (5, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (4, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (3, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (4, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (4, 3)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (4, 3)\n",
      "\t\t\treward: -1\n",
      "state: (3, 4)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (4, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (3, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (2, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (3, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (3, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (3, 4)\n",
      "\t\t\treward: -1\n",
      "state: (2, 4)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (3, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (2, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (1, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (2, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (2, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (2, 4)\n",
      "\t\t\treward: -1\n",
      "state: (8, 4)\n",
      "\taction: (1, 0)\n",
      "\t\tnext state: (9, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 1)\n",
      "\t\tnext state: (8, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (-1, 0)\n",
      "\t\tnext state: (7, 4)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, -1)\n",
      "\t\tnext state: (8, 3)\n",
      "\t\t\treward: -1\n",
      "\t\tnext state: (8, 4)\n",
      "\t\t\treward: -1\n",
      "\taction: (0, 0)\n",
      "\t\tnext state: (8, 4)\n",
      "\t\t\treward: -1\n"
     ]
    }
   ],
   "source": [
    "viewDictionaryStructure(rewards_EnvC_PrefMLK[('L', 'K')], levelsReward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Belief Rewards Rho(b, s, a, s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:00:01.482016Z",
     "start_time": "2019-02-12T16:00:01.479563Z"
    }
   },
   "outputs": [],
   "source": [
    "def getBeliefReward(worldBeliefs, worldRewardFunctions, state, action, nextState):\n",
    "    beliefReward = sum([worldBeliefs[y]*worldRewardFunctions[y][state][action][nextState] for y in worldBeliefs])\n",
    "    return(beliefReward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T15:59:37.685635Z",
     "start_time": "2019-02-12T15:59:37.683141Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "impartial_b = {world: 1/6 for world in itertools.permutations(\"KLM\", 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:00:25.349029Z",
     "start_time": "2019-02-12T16:00:25.346329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499.9999999999999"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getBeliefReward(impartial_b, rewards_EnvC_PrefMLK, (0,0), (0,0), (0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Belief Updating: Tau(b, a, b') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ b_t(y) \\propto p(o_t|x_t, y, env)b_{t-1}(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T00:56:22.869012Z",
     "start_time": "2019-02-12T00:56:22.863926Z"
    }
   },
   "source": [
    "## constructing P(o|x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createObservations(environment, trueWorldInLocation, stateSet):\n",
    "    #{world : position state : possible goal: indicator of whether that possible goal is visible in that state/world combo}\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
