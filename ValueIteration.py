import math

class BoltzmannValueIteration(object):
    def __init__(self, transitionTable, rewardTable, valueTable, convergenceTolerance, discountingFactor, beta):
        self.transitionTable = transitionTable
        self.rewardTable  = rewardTable
        self.valueTable = valueTable
        self.convergenceTolerance = convergenceTolerance
        self.gamma = discountingFactor
        self.beta = beta

    def __call__(self):
        
        delta = self.convergenceTolerance*100
        while(delta > self.convergenceTolerance):
            delta = 0
            for state, actionDict in self.transitionTable.items():
                valueOfStateAtTimeT = self.valueTable[state]
                qforAllActions = [self.getQValue(state, action) for action in actionDict.keys()]
                self.valueTable[state] = max(qforAllActions) 
                delta = max(delta, abs(valueOfStateAtTimeT-self.valueTable[state]))
        policyTable = {state:self.getBoltzmannPolicy(state) for state in self.transitionTable.keys()}

        return([self.valueTable, policyTable])
    
    def getBoltzmannPolicy(self, state):
        exponents = [self.beta*self.getQValue(state, action) for action in self.transitionTable[state].keys()]
        actions = [action for action in self.transitionTable[state].keys()]

        # Scale to [0,700] if there are exponents larger than 700
        if len([exponent for exponent in exponents if exponent>700])>0:
            print("scaling exponents to [0,700]... On State:")
            print(state)
            exponents = [700*(exponent/max(exponents)) for exponent in exponents]

        statePolicy = {action: math.exp(exponent) for exponent, action in zip(exponents,actions)}
        normalizedPolicy = self.normalizeDictionaryValues(statePolicy)
        return(normalizedPolicy)

    def getBoltzmannPolicy_NotAdjustedForMathOverflows(self, state):
        statePolicy = {action: math.exp(self.beta*self.getQValue(state, action)) for action in self.transitionTable[state].keys()}
        normalizedPolicy = self.normalizeDictionaryValues(statePolicy)
        return(normalizedPolicy)

    def getQValue(self, state, action):
        nextStatesQ = [prob*(self.rewardTable[state][action][nextState] \
                             + self.gamma*self.valueTable[nextState]) \
                      for nextState, prob in self.transitionTable[state][action].items()]
        qValue = sum(nextStatesQ)
        return(qValue)
    
    def normalizeDictionaryValues(self, unnormalizedDictionary):
        totalSum = sum(unnormalizedDictionary.values())
        normalizedDictionary = {originalKey: val/totalSum for originalKey, val in unnormalizedDictionary.items()}
        return(normalizedDictionary)

class DeterministicValueIteration(object):
    def __init__(self, transitionTable, rewardTable, valueTable, convergenceTolerance, discountingFactor = .99):
        self.transitionTable = transitionTable
        self.rewardTable  = rewardTable
        self.valueTable = valueTable
        self.convergenceTolerance = convergenceTolerance
        self.gamma = discountingFactor

    def __call__(self):
        theta = self.convergenceTolerance*100
        while(theta > self.convergenceTolerance):
            theta = 0
            for state, actionDict in self.transitionTable.items():

                valueOfStateAtTimeT = self.valueTable[state]
                self.valueTable[state] = max([self.getQValue(state, action) for action in actionDict.keys()])
                theta = max(theta, abs(valueOfStateAtTimeT-self.valueTable[state]))

        policyTable = {state:self.getStatePolicy(state) for state in self.transitionTable.keys()}
        return([self.valueTable, policyTable])
    
    def getStatePolicy(self, state):
        roundingThreshold = 5

        maxQValue = max([round(self.getQValue(state, action),roundingThreshold) for action in self.transitionTable[state].keys()])
        optimalActionSet = [action for action in self.transitionTable[state].keys() \
                            if round(self.getQValue(state, action),roundingThreshold) == maxQValue]
        statePolicy = {action: 1/(len(optimalActionSet)) for action in optimalActionSet}
        return(statePolicy)
        
    def getQValue(self, state, action):
        nextStatesQ = [prob*(self.rewardTable[state][action][nextState] \
                             + self.gamma*self.valueTable[nextState]) \
                      for nextState, prob in self.transitionTable[state][action].items()]
        qValue = sum(nextStatesQ)
        return(qValue)


def main():

    import numpy as np
    
    #Example 3: Testing Example
    #Checks whether code outputs values equivalent to a given optimal value / policy table
    convergence = .000001
    gamma = .9
    transition = {(0, 0): {(1, 0): {(1, 0): 0.7, (0, 1): 0.2, (0, 0): 0.1},(0, 1): {(0, 1): 0.7999999999999999, (1, 0): 0.2},(-1, 0): {(0, 0): 0.7, (1, 0): 0.2, (0, 1): 0.1},(0, -1): {(0, 0): 0.7, (1, 0): 0.1, (0, 1): 0.2}},(0, 1): {(1, 0): {(1, 1): 0.7999999999999999, (0, 1): 0.1, (0, 2): 0.1},(0, 1): {(0, 2): 0.7999999999999999, (0, 0): 0.2},(-1, 0): {(0, 1): 0.8999999999999999, (0, 0): 0.1},(0, -1): {(0, 0): 0.7999999999999999, (0, 2): 0.1, (0, 1): 0.1}},(0, 2): {(1, 0): {(1, 2): 0.7999999999999999, (0, 1): 0.2},(0, 1): {(0, 3): 0.7999999999999999, (0, 1): 0.1, (1, 2): 0.1},(-1, 0): {(0, 2): 0.7, (0, 1): 0.1, (1, 2): 0.1, (0, 3): 0.1},(0, -1): {(0, 1): 0.8999999999999999, (0, 3): 0.1}},(0, 3): {(1, 0): {(1, 3): 0.8999999999999999, (0, 2): 0.1},(0, 1): {(0, 4): 0.7999999999999999, (0, 3): 0.2},(-1, 0): {(0, 3): 0.8999999999999999, (0, 4): 0.1},(0, -1): {(0, 2): 0.8999999999999999, (1, 3): 0.1}},(0, 4): {(1, 0): {(1, 4): 0.7, (0, 3): 0.30000000000000004},(0, 1): {(0, 4): 0.7, (0, 3): 0.2, (1, 4): 0.1},(-1, 0): {(0, 4): 0.8999999999999999, (1, 4): 0.1},(0, -1): {(0, 3): 0.7, (1, 4): 0.2, (0, 4): 0.1}},(1, 0): {(1, 0): {(2, 0): 0.7, (1, 0): 0.2, (0, 0): 0.1},(0, 1): {(1, 1): 0.7999999999999999, (2, 0): 0.1, (1, 0): 0.1},(-1, 0): {(0, 0): 0.7, (1, 1): 0.2, (2, 0): 0.1},(0, -1): {(1, 0): 0.7, (2, 0): 0.1, (0, 0): 0.2}},(1, 1): {(1, 0): {(2, 1): 0.7999999999999999, (1, 0): 0.2},(0, 1): {(1, 2): 0.7, (0, 1): 0.30000000000000004},(-1, 0): {(0, 1): 0.7999999999999999, (2, 1): 0.2},(0, -1): {(1, 0): 0.7999999999999999, (2, 1): 0.1, (1, 2): 0.1}},(1, 2): {(1, 0): {(2, 2): 0.7, (1, 3): 0.2, (1, 1): 0.1},(0, 1): {(1, 3): 0.7999999999999999, (1, 1): 0.2},(-1, 0): {(0, 2): 0.7999999999999999, (1, 3): 0.2},(0, -1): {(1, 1): 0.8999999999999999, (2, 2): 0.1}},(1, 3): {(1, 0): {(2, 3): 0.7999999999999999, (1, 4): 0.2},(0, 1): {(1, 4): 0.7999999999999999, (2, 3): 0.1, (0, 3): 0.1},(-1, 0): {(0, 3): 0.7999999999999999, (2, 3): 0.2},(0, -1): {(1, 2): 0.7999999999999999, (0, 3): 0.1, (1, 4): 0.1}},(1, 4): {(1, 0): {(2, 4): 0.8999999999999999, (1, 4): 0.1},(0, 1): {(1, 4): 0.7999999999999999, (1, 3): 0.1, (0, 4): 0.1},(-1, 0): {(0, 4): 0.8999999999999999, (1, 4): 0.1},(0, -1): {(1, 3): 0.7999999999999999, (0, 4): 0.2}},(2, 0): {(1, 0): {(3, 0): 0.7999999999999999, (2, 0): 0.2},(0, 1): {(2, 1): 0.8999999999999999, (2, 0): 0.1},(-1, 0): {(1, 0): 0.7, (2, 0): 0.2, (2, 1): 0.1},(0, -1): {(2, 0): 0.7, (1, 0): 0.1, (3, 0): 0.2}},(2, 1): {(1, 0): {(3, 1): 0.7999999999999999, (1, 1): 0.1, (2, 0): 0.1},(0, 1): {(2, 2): 0.7, (3, 1): 0.2, (1, 1): 0.1},(-1, 0): {(1, 1): 0.7999999999999999, (3, 1): 0.2},(0, -1): {(2, 0): 0.7999999999999999, (3, 1): 0.2}},(2, 2): {(1, 0): {(3, 2): 0.7999999999999999, (2, 1): 0.1, (1, 2): 0.1},(0, 1): {(2, 3): 0.8999999999999999, (1, 2): 0.1},(-1, 0): {(1, 2): 0.7, (3, 2): 0.1, (2, 3): 0.1, (2, 1): 0.1},(0, -1): {(2, 1): 0.7999999999999999, (1, 2): 0.2}},(2, 3): {(1, 0): {(3, 3): 0.7999999999999999, (2, 4): 0.2},(0, 1): {(2, 4): 0.7999999999999999, (2, 2): 0.2},(-1, 0): {(1, 3): 0.7999999999999999, (2, 4): 0.1, (2, 2): 0.1},(0, -1): {(2, 2): 0.7999999999999999, (2, 4): 0.2}},(2, 4): {(1, 0): {(3, 4): 0.7, (2, 3): 0.2, (1, 4): 0.1},(0, 1): {(2, 4): 0.7999999999999999, (3, 4): 0.1, (2, 3): 0.1},(-1, 0): {(1, 4): 0.7999999999999999, (3, 4): 0.2},(0, -1): {(2, 3): 0.7999999999999999, (3, 4): 0.1, (1, 4): 0.1}},(3, 0): {(1, 0): {(4, 0): 0.7, (3, 0): 0.2, (2, 0): 0.1},(0, 1): {(3, 1): 0.7999999999999999, (4, 0): 0.2},(-1, 0): {(2, 0): 0.8999999999999999, (3, 0): 0.1},(0, -1): {(3, 0): 0.9999999999999999}},(3, 1): {(1, 0): {(4, 1): 0.7, (2, 1): 0.1, (3, 2): 0.1, (3, 0): 0.1},(0, 1): {(3, 2): 0.7999999999999999, (4, 1): 0.1, (3, 0): 0.1},(-1, 0): {(2, 1): 0.7999999999999999, (4, 1): 0.1, (3, 0): 0.1},(0, -1): {(3, 0): 0.7, (3, 2): 0.2, (4, 1): 0.1}},(3, 2): {(1, 0): {(4, 2): 0.7, (3, 1): 0.2, (3, 3): 0.1},(0, 1): {(3, 3): 0.7, (4, 2): 0.30000000000000004},(-1, 0): {(2, 2): 0.7, (3, 3): 0.2, (3, 1): 0.1},(0, -1): {(3, 1): 0.7, (3, 3): 0.1, (4, 2): 0.2}},(3, 3): {(1, 0): {(4, 3): 0.8999999999999999, (3, 2): 0.1},(0, 1): {(3, 4): 0.9999999999999999},(-1, 0): {(2, 3): 0.7999999999999999, (3, 4): 0.1, (3, 2): 0.1},(0, -1): {(3, 2): 0.7, (3, 4): 0.1, (4, 3): 0.2}},(3, 4): {(1, 0): {(4, 4): 0.8999999999999999, (2, 4): 0.1},(0, 1): {(3, 4): 0.7, (3, 3): 0.1, (2, 4): 0.1, (4, 4): 0.1},(-1, 0): {(2, 4): 0.7, (3, 4): 0.2, (4, 4): 0.1},(0, -1): {(3, 3): 0.7999999999999999, (2, 4): 0.1, (3, 4): 0.1}},(4, 0): {(1, 0): {(4, 0): 0.7999999999999999, (4, 1): 0.1, (3, 0): 0.1},(0, 1): {(4, 1): 0.8999999999999999, (4, 0): 0.1},(-1, 0): {(3, 0): 0.7, (4, 0): 0.2, (4, 1): 0.1},(0, -1): {(4, 0): 0.7, (4, 1): 0.30000000000000004}},(4, 1): {(1, 0): {(4, 1): 0.7, (4, 2): 0.1, (3, 1): 0.1, (4, 0): 0.1},(0, 1): {(4, 2): 0.7, (4, 0): 0.2, (3, 1): 0.1},(-1, 0): {(3, 1): 0.7999999999999999, (4, 0): 0.2},(0, -1): {(4, 0): 0.7999999999999999, (4, 2): 0.1, (4, 1): 0.1}},(4, 2): {(1, 0): {(4, 2): 0.8999999999999999, (4, 3): 0.1},(0, 1): {(4, 3): 0.7, (4, 1): 0.1, (4, 2): 0.2},(-1, 0): {(3, 2): 0.7999999999999999, (4, 1): 0.1, (4, 3): 0.1},(0, -1): {(4, 1): 0.7999999999999999, (4, 3): 0.1, (4, 2): 0.1}},(4, 3): {(1, 0): {(4, 3): 0.8999999999999999, (4, 4): 0.1},(0, 1): {(4, 4): 0.7999999999999999, (4, 3): 0.2},(-1, 0): {(3, 3): 0.7, (4, 2): 0.2, (4, 4): 0.1},(0, -1): {(4, 2): 0.7999999999999999, (3, 3): 0.2}},(4, 4): {(1, 0): {(4, 4): 0.8999999999999999, (4, 3): 0.1},(0, 1): {(4, 4): 0.7999999999999999, (3, 4): 0.2},(-1, 0): {(3, 4): 0.7999999999999999, (4, 3): 0.2},(0, -1): {(4, 3): 0.7999999999999999, (4, 4): 0.1, (3, 4): 0.1}}}
    reward = {(0, 0): {(1, 0): {(1, 0): -1, (0, 1): -1, (0, 0): -1},(0, 1): {(0, 1): -1, (1, 0): -1},(-1, 0): {(0, 0): -1, (1, 0): -1, (0, 1): -1},(0, -1): {(0, 0): -1, (1, 0): -1, (0, 1): -1}},(0, 1): {(1, 0): {(1, 1): -1, (0, 1): -1, (0, 2): -1},(0, 1): {(0, 2): -1, (0, 0): -1},(-1, 0): {(0, 1): -1, (0, 0): -1},(0, -1): {(0, 0): -1, (0, 2): -1, (0, 1): -1}},(0, 2): {(1, 0): {(1, 2): -1, (0, 1): -1},(0, 1): {(0, 3): -1, (0, 1): -1, (1, 2): -1},(-1, 0): {(0, 2): -1, (0, 1): -1, (1, 2): -1, (0, 3): -1},(0, -1): {(0, 1): -1, (0, 3): -1}},(0, 3): {(1, 0): {(1, 3): -1, (0, 2): -1},(0, 1): {(0, 4): -1, (0, 3): -1},(-1, 0): {(0, 3): -1, (0, 4): -1},(0, -1): {(0, 2): -1, (1, 3): -1}},(0, 4): {(1, 0): {(1, 4): -1, (0, 3): -1},(0, 1): {(0, 4): -1, (0, 3): -1, (1, 4): -1},(-1, 0): {(0, 4): -1, (1, 4): -1},(0, -1): {(0, 3): -1, (1, 4): -1, (0, 4): -1}},(1, 0): {(1, 0): {(2, 0): -1, (1, 0): -1, (0, 0): -1},(0, 1): {(1, 1): -1, (2, 0): -1, (1, 0): -1},(-1, 0): {(0, 0): -1, (1, 1): -1, (2, 0): -1},(0, -1): {(1, 0): -1, (2, 0): -1, (0, 0): -1}},(1, 1): {(1, 0): {(2, 1): -100, (1, 0): -100},(0, 1): {(1, 2): -100, (0, 1): -100},(-1, 0): {(0, 1): -100, (2, 1): -100},(0, -1): {(1, 0): -100, (2, 1): -100, (1, 2): -100}},(1, 2): {(1, 0): {(2, 2): -1, (1, 3): -1, (1, 1): -1},(0, 1): {(1, 3): -1, (1, 1): -1},(-1, 0): {(0, 2): -1, (1, 3): -1},(0, -1): {(1, 1): -1, (2, 2): -1}},(1, 3): {(1, 0): {(2, 3): -1, (1, 4): -1},(0, 1): {(1, 4): -1, (2, 3): -1, (0, 3): -1},(-1, 0): {(0, 3): -1, (2, 3): -1},(0, -1): {(1, 2): -1, (0, 3): -1, (1, 4): -1}},(1, 4): {(1, 0): {(2, 4): -1, (1, 4): -1},(0, 1): {(1, 4): -1, (1, 3): -1, (0, 4): -1},(-1, 0): {(0, 4): -1, (1, 4): -1},(0, -1): {(1, 3): -1, (0, 4): -1}},(2, 0): {(1, 0): {(3, 0): -1, (2, 0): -1},(0, 1): {(2, 1): -1, (2, 0): -1},(-1, 0): {(1, 0): -1, (2, 0): -1, (2, 1): -1},(0, -1): {(2, 0): -1, (1, 0): -1, (3, 0): -1}},(2, 1): {(1, 0): {(3, 1): -1, (1, 1): -1, (2, 0): -1},(0, 1): {(2, 2): -1, (3, 1): -1, (1, 1): -1},(-1, 0): {(1, 1): -1, (3, 1): -1},(0, -1): {(2, 0): -1, (3, 1): -1}},(2, 2): {(1, 0): {(3, 2): -1, (2, 1): -1, (1, 2): -1},(0, 1): {(2, 3): -1, (1, 2): -1},(-1, 0): {(1, 2): -1, (3, 2): -1, (2, 3): -1, (2, 1): -1},(0, -1): {(2, 1): -1, (1, 2): -1}},(2, 3): {(1, 0): {(3, 3): -1, (2, 4): -1},(0, 1): {(2, 4): -1, (2, 2): -1},(-1, 0): {(1, 3): -1, (2, 4): -1, (2, 2): -1},(0, -1): {(2, 2): -1, (2, 4): -1}},(2, 4): {(1, 0): {(3, 4): -1, (2, 3): -1, (1, 4): -1},(0, 1): {(2, 4): -1, (3, 4): -1, (2, 3): -1},(-1, 0): {(1, 4): -1, (3, 4): -1},(0, -1): {(2, 3): -1, (3, 4): -1, (1, 4): -1}},(3, 0): {(1, 0): {(4, 0): -1, (3, 0): -1, (2, 0): -1},(0, 1): {(3, 1): -1, (4, 0): -1},(-1, 0): {(2, 0): -1, (3, 0): -1},(0, -1): {(3, 0): -1}},(3, 1): {(1, 0): {(4, 1): 10, (2, 1): 10, (3, 2): 10, (3, 0): 10},(0, 1): {(3, 2): 10, (4, 1): 10, (3, 0): 10},(-1, 0): {(2, 1): 10, (4, 1): 10, (3, 0): 10},(0, -1): {(3, 0): 10, (3, 2): 10, (4, 1): 10}},(3, 2): {(1, 0): {(4, 2): -1, (3, 1): -1, (3, 3): -1},(0, 1): {(3, 3): -1, (4, 2): -1},(-1, 0): {(2, 2): -1, (3, 3): -1, (3, 1): -1},(0, -1): {(3, 1): -1, (3, 3): -1, (4, 2): -1}},(3, 3): {(1, 0): {(4, 3): -1, (3, 2): -1},(0, 1): {(3, 4): -1},(-1, 0): {(2, 3): -1, (3, 4): -1, (3, 2): -1},(0, -1): {(3, 2): -1, (3, 4): -1, (4, 3): -1}},(3, 4): {(1, 0): {(4, 4): -1, (2, 4): -1},(0, 1): {(3, 4): -1, (3, 3): -1, (2, 4): -1, (4, 4): -1},(-1, 0): {(2, 4): -1, (3, 4): -1, (4, 4): -1},(0, -1): {(3, 3): -1, (2, 4): -1, (3, 4): -1}},(4, 0): {(1, 0): {(4, 0): -1, (4, 1): -1, (3, 0): -1},(0, 1): {(4, 1): -1, (4, 0): -1},(-1, 0): {(3, 0): -1, (4, 0): -1, (4, 1): -1},(0, -1): {(4, 0): -1, (4, 1): -1}},(4, 1): {(1, 0): {(4, 1): -1, (4, 2): -1, (3, 1): -1, (4, 0): -1},(0, 1): {(4, 2): -1, (4, 0): -1, (3, 1): -1},(-1, 0): {(3, 1): -1, (4, 0): -1},(0, -1): {(4, 0): -1, (4, 2): -1, (4, 1): -1}},(4, 2): {(1, 0): {(4, 2): -1, (4, 3): -1},(0, 1): {(4, 3): -1, (4, 1): -1, (4, 2): -1},(-1, 0): {(3, 2): -1, (4, 1): -1, (4, 3): -1},(0, -1): {(4, 1): -1, (4, 3): -1, (4, 2): -1}},(4, 3): {(1, 0): {(4, 3): -1, (4, 4): -1},(0, 1): {(4, 4): -1, (4, 3): -1},(-1, 0): {(3, 3): -1, (4, 2): -1, (4, 4): -1},(0, -1): {(4, 2): -1, (3, 3): -1}},(4, 4): {(1, 0): {(4, 4): -1, (4, 3): -1},(0, 1): {(4, 4): -1, (3, 4): -1},(-1, 0): {(3, 4): -1, (4, 3): -1},(0, -1): {(4, 3): -1, (4, 4): -1, (3, 4): -1}}}
    value = {state:0 for state in reward.keys()}
    performValueIteration = DeterministicValueIteration(transition, reward, value, convergence, gamma)
    optimalValuesTest, policyTest = performValueIteration()

    optimalValues = {(0, 0): 15.025707222911826,(0, 1): 11.731234437348668,(0, 2): 9.521269745627714,(0, 3): 11.32188785424639,(0, 4): 9.68215884553303,(1, 0): 19.939320800400704,(1, 1): -79.16973872862285,(1, 2): 8.730406103922434,(1, 3): 14.154288919093116,(1, 4): 12.10357054578651,(2, 0): 25.393539662324756,(2, 1): 23.946088854204383,(2, 2): 22.710969889175047,(2, 3): 18.021731485146322,(2, 4): 14.83240692319781,(3, 0): 30.30930984335257,(3, 1): 37.01522442294975,(3, 2): 28.847341249549103,(3, 3): 22.604168958415382,(3, 4): 18.252657402682686,(4, 0): 25.87971530013074,(4, 1): 30.309310338547352,(4, 2): 24.963193213128918,(4, 3): 21.04224952596759,(4, 4): 17.35511955127787}
    policy = {(0, 0): {(1, 0): 1.0},(0, 1): {(0, -1): 1.0},(0, 2): {(0, -1): 1.0},(0, 3): {(1, 0): 1.0},(0, 4): {(1, 0): 1.0},(1, 0): {(1, 0): 1.0},(1, 1): {(1, 0): 1.0},(1, 2): {(1, 0): 1.0},(1, 3): {(1, 0): 1.0},(1, 4): {(1, 0): 1.0},(2, 0): {(1, 0): 1.0},(2, 1): {(0, -1): 1.0},(2, 2): {(1, 0): 1.0},(2, 3): {(0, -1): 1.0},(2, 4): {(1, 0): 1.0},(3, 0): {(0, 1): 1.0},(3, 1): {(0, -1): 1.0},(3, 2): {(0, -1): 1.0},(3, 3): {(0, -1): 1.0},(3, 4): {(0, -1): 1.0},(4, 0): {(0, 1): 1.0},(4, 1): {(-1, 0): 1.0},(4, 2): {(0, -1): 1.0},(4, 3): {(0, -1): 1.0},(4, 4): {(0, -1): 1.0}}

    np.testing.assert_approx_equal(optimalValues[(4,4)], optimalValuesTest[(4,4)], significant=3 ,err_msg='',verbose=True)
    np.testing.assert_approx_equal(optimalValues[(3,1)], optimalValuesTest[(3,1)], significant=3 ,err_msg='',verbose=True)
    np.testing.assert_approx_equal(optimalValues[(1,2)], optimalValuesTest[(1,1)], significant=3 ,err_msg='',verbose=True)
    np.testing.assert_equal(policy,policyTest)


if __name__ == '__main__':
    main()